distribution="posterior"))
ggplot(data_plot)+
geom_line(aes(x=mu,y=density,color=distribution),size=2)+
theme_minimal(base_size = 14)+xlab(expression(mu))+ylab("Density")
library(ggplot2)
grid_mu <- seq(100,200,length.out = 1000)
# priori para mu (altura média de um brasileiro)
mu0 <- 165
sigma0 <- 10
tau0_2 <- 1/(sigma0)^2
data_plot <- data.frame(mu=grid_mu,
density=dnorm(grid_mu,mu0,sigma0),
distribution="priori")
# dados (altura medida de um indivíduo - normal com média conhecida)
x <- 180
sigma <- 5
tau_2 <- 1/(sigma)^2
# posteriori
tau_linha_2 <- tau0_2 + tau_2
mu_linha <- tau0_2/tau_linha_2*mu0+
tau_2/tau_linha_2*x
data_plot <- rbind(data_plot,
data.frame(mu=grid_mu,
density=dnorm(grid_mu,mu_linha,1/sqrt(tau_linha_2)),
distribution="posteriori"))
ggplot(data_plot)+
geom_line(aes(x=mu,y=density,color=distribution),size=1.2)+
theme_minimal(base_size = 14)+xlab(expression(mu))+ylab("Densidade")+
theme(legend.title=element_blank())
# X ~ N(mu,sigma)
sigma <- 7.5
mu <- 170
alturas <- rnorm(100,mu,sigma)
hist(alturas)
# Frequentista, EMV
mean(alturas)
#  Bayesiano
mu0 <- 165
lambda <- 1
grid_tau <- seq(0,0.1,length.out = 10000)
beta <- 1
alpha <- 0.1*beta
hist(rgamma(1000,
alpha,beta))
plot(grid_tau,dgamma(grid_tau,
alpha,beta))
hist(rgamma(1000,
alpha,beta))
plot(grid_tau,dgamma(grid_tau,
alpha,beta))
grid_tau <- seq(0,0.01,length.out = 10000)
beta <- 1
alpha <- 0.1*beta
plot(grid_tau,dgamma(grid_tau,
alpha,beta))
?dgamma
# X ~ N(mu,sigma)
sigma <- 0.75
mu <- 1.70
alturas <- rnorm(100,mu,sigma)
hist(alturas)
grid_tau <- seq(0,0.01,
length.out = 10000)
beta <- 1
alpha <- 100*beta
plot(grid_tau,dgamma(grid_tau,
alpha,beta))
grid_tau <- seq(0,300,
length.out = 10000)
beta <- 1
alpha <- 100*beta
plot(grid_tau,dgamma(grid_tau,
alpha,beta))
library(FNN)
library(fields)
library(nnet)
library(gmodels)
library(randomForest)
# Gera dados
nTrain=2000
library(FNN)
library(fields)
library(nnet)
library(gmodels)
install.packages("gmodels")
library(gmodels)
library(randomForest)
# Gera dados
nTrain=2000
nTrain0=4000
n=nTrain+nTrain0
d=1
data=matrix(NA,n,d+1)
data[,1:d]=matrix(rnorm(n*d,0,1),n,d)
#Número de categorias resposta-1
p=4
#Betas
beta=matrix(NA,nrow=p,ncol=1)
set.seed(3)
beta = rnorm(p,0,5)
#Função variável resposta
resp = function(x,p){
Denominator=1
for(i in 1:p){Denominator=Denominator+exp(beta[i]*x)}
vProb = cbind(1/Denominator,exp(x*beta[1])/Denominator)
if (p>1) {for(i in 2:p){vProb = cbind(vProb,exp(x*beta[i])/Denominator)}}
mChoices = t(apply(vProb, 1, rmultinom, n = 1, size = 1))#gera multinomial
return(apply(mChoices, 1, function(x) which(x==1)))
}
#Função Qtd diferente para resumo
qtd_dif=function(x){length(unique(x))}
data[,d+1]=resp(x=data[,1:d],p=p)
table(data[,d+1])
plot(data[,1],data[,d+1],pch=20,xlim=c(-5,5),xlab="X",ylab="Y")
plot(table(data[,d+1]),xlab="y",ylab="Frequência")
randomIndex=sample(1:n)
xTrain=data[randomIndex[1:nTrain],1:d]
xTrain0=data[randomIndex[(nTrain+1):n],1:d]
yTrain=data[randomIndex[1:nTrain],d+1]
yTrain0=data[randomIndex[(nTrain+1):n],d+1]
#Função distribuição probabilidade
dens = function(x,y){
Denominator=1
for(i in 1:p){Denominator=Denominator+exp(beta[i]*x)}
vProb = cbind(1/Denominator,exp(x*beta[1])/Denominator)
if (p>1) {for(i in 2:p){vProb = cbind(vProb,exp(x*beta[i])/Denominator)}}
return(c(vProb[y]))
}
fit<-(multinom(yTrain0 ~ xTrain0, cbind.data.frame(yTrain0,xTrain0)))
predictedValues=predict(fit, newdata = data.frame(xTrain0=xTrain),type = "probs")
predictedValues[1:5,]
fit<-randomForest(y=as.factor(yTrain0),x=as.matrix(xTrain0))
predictedValues=predict(fit, x=as.matrix(xTrain),type = "prob")
predictedValues[1:5,]
knn.indice = function(xTrain,xTest,k){
n.test = nrow(as.matrix(xTest))
viz.test = matrix(0, nrow = n.test, ncol = k)
dist.train = rdist(xTest,xTrain)
for(i in 1:n.test){
dist = dist.train[i,]
viz.test[i,] = order(dist)[1:(k)]
}
return(viz.test)
}
cplocaldist = function(k,alfa){
#Variável explicativa
data[,1:d]=matrix(rnorm(n*d,0,1),n,d)
# Variável resposta
data[,d+1]=resp(x=data[,1:d],p=p)
# split data (sem index)
xTrain=data[randomIndex[1:nTrain],1:d]
yTrain=data[randomIndex[1:nTrain],d+1]
#Densidade estimada para novo xTrain
#pred_xTrain=predict(fit, newdata = data.frame(xTrain0=xTrain),type = "probs")
pred_xTrain=predict(fit, x=as.matrix(xTrain),type = "prob")
#grid y
y_grid=c(1:(p+1))
#grid x
xTest_grid=seq(-4,4, length.out = 2000)
#Gera y corresponde ao x grid
yTest_grid=resp(x=xTest_grid,p=p)
#Densidade estimada para xTest_grid
#pred_xTest_grid=predict(fit, newdata = data.frame(xTrain0=xTest_grid),type = "probs")
pred_xTest_grid=predict(fit, x=as.matrix(xTest_grid),type = "prob")
#g train e g test
#t_grid=seq(0,1,length.out = 500)
#gx_train=matrix(NA, nrow = nrow(as.matrix(xTrain)), ncol=length(t_grid))
#gx_test=matrix(NA, nrow = nrow(as.matrix(xTest_grid)), ncol=length(t_grid))
gx_train=matrix(NA, nrow = nrow(as.matrix(xTrain)), ncol=p+1)
gx_test=matrix(NA, nrow = nrow(as.matrix(xTest_grid)), ncol=p+1)
#for(i in 1:nrow(as.matrix(xTest_grid))){gx_test[i,]=g(t_grid,y_grid,pred_xTest_grid[i,] )}  #estimativa f para XText_grid
#for(i in 1:nrow(as.matrix(xTrain))){gx_train[i,]=g(t_grid,y_grid,pred_xTrain[i,] )}  #usa estimativa f para xTrain
for(i in 1:nrow(as.matrix(xTest_grid))){gx_test[i,]=pred_xTest_grid[i,]}  #estimativa f para XText_grid
for(i in 1:nrow(as.matrix(xTrain))){gx_train[i,]=pred_xTrain[i,]}  #usa estimativa f para xTrain
#elementos do treino próximos de cada teste
x_test_viz=knn.indice(gx_train,gx_test,k)
#percentil 1-alfa para cada teste
mat_cde_kviz=matrix(NA, nrow = nrow(as.matrix(xTest_grid)), ncol=k)
p_alfa=matrix(NA, nrow = nrow(as.matrix(xTest_grid)), ncol=1)
for(i in 1:nrow(as.matrix(xTest_grid))){
for(j in 1:k){mat_cde_kviz[i,j]=pred_xTrain[x_test_viz[i,j],yTrain[x_test_viz[i,j]]]}} #estimativa f
for(i in 1:nrow(as.matrix(xTest_grid))){p_alfa[i]=quantile(mat_cde_kviz[i,],prob=alfa)}
#indicador de cobertura e comprimento
Ind=matrix(NA, nrow = length(xTest_grid))
xTestBand = matrix(0, nrow = length(xTest_grid), ncol=p+1)
Comp=matrix(NA, nrow = nrow(as.matrix(xTest_grid)))
for(i in 1:length(xTest_grid)){
xTestBand[i,]=ifelse(pred_xTest_grid[i,]>=p_alfa[i],y_grid,NA) #estimativa f
Ind[i]=ifelse(pred_xTest_grid[i,yTest_grid[i]]>=p_alfa[i],1,0) #estimativa f
Comp[i]=sum(abs(diff(xTestBand[i,])),na.rm=TRUE)
}
return(list(Ind,Comp,xTestBand))
}
bandapred=cplocaldist(k=100,alfa = 0.1)
mean(bandapred[[1]])
mean(bandapred[[2]])
bandapred[[1]]
hist(bandapred[[1]])
hist((bandapred[[2]])
)
library(FNN)
library(fields)
library(nnet)
library(gmodels)
library(randomForest)
# Gera dados
nTrain=2000
nTrain0=4000
n=nTrain+nTrain0
d=1
data=matrix(NA,n,d+1)
data[,1:d]=matrix(rnorm(n*d,0,1),n,d)
#Número de categorias resposta-1
p=4
#Betas
beta=matrix(NA,nrow=p,ncol=1)
set.seed(3)
beta = rnorm(p,0,5)
#Função variável resposta
resp = function(x,p){
Denominator=1
for(i in 1:p){Denominator=Denominator+exp(beta[i]*x)}
vProb = cbind(1/Denominator,exp(x*beta[1])/Denominator)
if (p>1) {for(i in 2:p){vProb = cbind(vProb,exp(x*beta[i])/Denominator)}}
mChoices = t(apply(vProb, 1, rmultinom, n = 1, size = 1))#gera multinomial
return(apply(mChoices, 1, function(x) which(x==1)))
}
#Função Qtd diferente para resumo
qtd_dif=function(x){length(unique(x))}
data[,d+1]=resp(x=data[,1:d],p=p)
table(data[,d+1])
plot(data[,1],data[,d+1],pch=20,xlim=c(-5,5),xlab="X",ylab="Y")
plot(table(data[,d+1]),xlab="y",ylab="Frequência")
randomIndex=sample(1:n)
xTrain=data[randomIndex[1:nTrain],1:d]
xTrain0=data[randomIndex[(nTrain+1):n],1:d]
yTrain=data[randomIndex[1:nTrain],d+1]
yTrain0=data[randomIndex[(nTrain+1):n],d+1]
#Função distribuição probabilidade
dens = function(x,y){
Denominator=1
for(i in 1:p){Denominator=Denominator+exp(beta[i]*x)}
vProb = cbind(1/Denominator,exp(x*beta[1])/Denominator)
if (p>1) {for(i in 2:p){vProb = cbind(vProb,exp(x*beta[i])/Denominator)}}
return(c(vProb[y]))
}
fit<-(multinom(yTrain0 ~ xTrain0, cbind.data.frame(yTrain0,xTrain0)))
predictedValues=predict(fit, newdata = data.frame(xTrain0=xTrain),type = "probs")
predictedValues[1:5,]
fit<-randomForest(y=as.factor(yTrain0),x=as.matrix(xTrain0))
predictedValues=predict(fit, x=as.matrix(xTrain),type = "prob")
predictedValues[1:5,]
knn.indice = function(xTrain,xTest,k){
n.test = nrow(as.matrix(xTest))
viz.test = matrix(0, nrow = n.test, ncol = k)
dist.train = rdist(xTest,xTrain)
for(i in 1:n.test){
dist = dist.train[i,]
viz.test[i,] = order(dist)[1:(k)]
}
return(viz.test)
}
cplocaldist = function(k,alfa){
#Variável explicativa
data[,1:d]=matrix(rnorm(n*d,0,1),n,d)
# Variável resposta
data[,d+1]=resp(x=data[,1:d],p=p)
# split data (sem index)
xTrain=data[randomIndex[1:nTrain],1:d]
yTrain=data[randomIndex[1:nTrain],d+1]
#Densidade estimada para novo xTrain
#pred_xTrain=predict(fit, newdata = data.frame(xTrain0=xTrain),type = "probs")
pred_xTrain=predict(fit, x=as.matrix(xTrain),type = "prob")
#grid y
y_grid=c(1:(p+1))
#grid x
xTest_grid=seq(-4,4, length.out = 2000)
#Gera y corresponde ao x grid
yTest_grid=resp(x=xTest_grid,p=p)
#Densidade estimada para xTest_grid
#pred_xTest_grid=predict(fit, newdata = data.frame(xTrain0=xTest_grid),type = "probs")
pred_xTest_grid=predict(fit, x=as.matrix(xTest_grid),type = "prob")
#g train e g test
#t_grid=seq(0,1,length.out = 500)
#gx_train=matrix(NA, nrow = nrow(as.matrix(xTrain)), ncol=length(t_grid))
#gx_test=matrix(NA, nrow = nrow(as.matrix(xTest_grid)), ncol=length(t_grid))
gx_train=matrix(NA, nrow = nrow(as.matrix(xTrain)), ncol=p+1)
gx_test=matrix(NA, nrow = nrow(as.matrix(xTest_grid)), ncol=p+1)
#for(i in 1:nrow(as.matrix(xTest_grid))){gx_test[i,]=g(t_grid,y_grid,pred_xTest_grid[i,] )}  #estimativa f para XText_grid
#for(i in 1:nrow(as.matrix(xTrain))){gx_train[i,]=g(t_grid,y_grid,pred_xTrain[i,] )}  #usa estimativa f para xTrain
for(i in 1:nrow(as.matrix(xTest_grid))){gx_test[i,]=pred_xTest_grid[i,]}  #estimativa f para XText_grid
for(i in 1:nrow(as.matrix(xTrain))){gx_train[i,]=pred_xTrain[i,]}  #usa estimativa f para xTrain
#elementos do treino próximos de cada teste
x_test_viz=knn.indice(gx_train,gx_test,k)
#percentil 1-alfa para cada teste
mat_cde_kviz=matrix(NA, nrow = nrow(as.matrix(xTest_grid)), ncol=k)
p_alfa=matrix(NA, nrow = nrow(as.matrix(xTest_grid)), ncol=1)
for(i in 1:nrow(as.matrix(xTest_grid))){
for(j in 1:k){mat_cde_kviz[i,j]=pred_xTrain[x_test_viz[i,j],yTrain[x_test_viz[i,j]]]}} #estimativa f
for(i in 1:nrow(as.matrix(xTest_grid))){p_alfa[i]=quantile(mat_cde_kviz[i,],prob=alfa)}
#indicador de cobertura e comprimento
Ind=matrix(NA, nrow = length(xTest_grid))
xTestBand = matrix(0, nrow = length(xTest_grid), ncol=p+1)
Comp=matrix(NA, nrow = nrow(as.matrix(xTest_grid)))
for(i in 1:length(xTest_grid)){
xTestBand[i,]=ifelse(pred_xTest_grid[i,]>=p_alfa[i],y_grid,NA) #estimativa f
Ind[i]=ifelse(pred_xTest_grid[i,yTest_grid[i]]>=p_alfa[i],1,0) #estimativa f
Comp[i]=sum(abs(diff(xTestBand[i,])),na.rm=TRUE)
}
return(list(Ind,Comp,xTestBand))
}
bandapred=cplocaldist(k=100,alfa = 0.1)
mean(bandapred[[1]])
mean(bandapred[[2]])
predictedValues
predictedValues[1:5,]
predictedValues[1:5,]
xTrain[1]
?randomForest
fit1<-randomForest(y=as.numeric(yTrain0==1),
x=as.matrix(xTrain0))
predictedValues1=predict(fit1, x=as.matrix(xTrain))
predictedValues1
predictedValues1[1]
xTrain[1]
fit2<-randomForest(y=as.numeric(yTrain0==2),
x=as.matrix(xTrain0))
predictedValues1=predict(fit2, x=as.matrix(xTrain))
predictedValues1[1]
predictedValues1=predict(fit1, x=as.matrix(xTrain))
predictedValues2=predict(fit2, x=as.matrix(xTrain))
predictedValues1[1]
predictedValues2[1]
predictedValues[1,]
fit5<-randomForest(y=as.numeric(yTrain0==5),
x=as.matrix(xTrain0))
predictedValues5=predict(fit5, x=as.matrix(xTrain))
predictedValues5[1]
rbind(predictedValues1,predictedValues2,predictedValues5)[1:5,]
cbind(predictedValues1,predictedValues2,predictedValues5)[1:5,]
predictedValues[1:5,]
xTrain[5]
mean(bandapred[[2]])
bandapred[[2]]
table(bandapred[[2]])
predictedValues[1:5,]
d <- seq(0,1,length.out = 1000)
d <- seq(0,1,length.out = 1000)
plot(d,dbeta(d,5,5),type="l",lwd=3,xlab=expression(theta))
plot(d,dbeta(d,5,5),type="l",lwd=3,xlab=expression(theta),ylab="Densidade")
plot(d,dbeta(d,5,5),type="l",lwd=3,xlab=expression(theta),ylab="Densidade",cex.lab=1.3)
x=10
n=30
lines(d,dbeta(d,5+x,5+n-x),col=2,lwd=3)
plot(d,dbeta(d,5,5),type="l",lwd=3,xlab=expression(theta),ylab="Densidade",
cex.lab=1.3,col=4,ylim=c(0,4))
x=10
n=15
lines(d,dbeta(d,5+x,5+n-x),col=2,lwd=3)
1-pbeta(0.5,25,15)
setwd("/mnt/62e7a18d-9e67-46b5-8b12-bcb7dc0301ab/Dropbox/Izbicki/Research/UFSCar/Gilson/conformal-cde-experiments/simulations")
source("../requirements.R")
source("../base_functions.R")
folder <- "../rds/gaussian_het/"
dir.create(folder, showWarnings = FALSE)
# if x is given, only generate response again
generate_het_gaussian <- function(n,d,x=NULL)
{
if(is.null(x))
{
x=matrix(runif(n*d,-5,5),n,d)
}
# response
y=rnorm(nrow(x),x[,1],abs(x[,1]))
return(list(x=x,y=y))
}
n_fits <- 25 # total numer of I1 datasets
n_repetitions <- 250 # total numer of I2 datasets
n_each_set_grid <- c(200,500,1000,2500,5000) # size of I1 and I2
n_test <- 500 # to check coverage
d <- 20
k <- 100
percent_train <- 0.7
alpha <- 0.1
generate_data <- function(n,x=NULL) {generate_het_gaussian(n=n,d=d,x=x)}
cd_split_global <- list()
cd_split_local <- list()
dist_split <- list()
quantile_split <- list()
reg_split <- list()
reg_split_w <- list()
for(n_each_index in 1:length(n_each_set_grid))
{
print(n_each_index/length(n_each_set_grid))
rep <- 1
bands_global <- list()
bands_local <- list()
bands_dist <- list()
bands_quantile <- list()
bands_reg <- list()
bands_reg_w <- list()
for(n_fits_index in 1:n_fits)
{
cat(".")
data_I1 <- generate_data(n=n_each_set_grid[n_each_index])
which_train <- sample(1:length(data_I1$y),length(data_I1$y)*percent_train)
cde_fit <- fit_density_forest(xTrain=data_I1$x[which_train,,drop=FALSE],
yTrain = data_I1$y[which_train,drop=FALSE],
xValidation=data_I1$x[-which_train,,drop=FALSE],
yValidation = data_I1$y[-which_train,drop=FALSE])
for(ll in 1:n_repetitions)
{
data_I2 <- generate_data(n=n_each_set_grid[n_each_index])
pred_I2 <- predict(cde_fit,data_I2$x)
t_grid <- seq(0,max(pred_I2$CDE),length.out = 500)
# Dist-split
fit_dist_split <- dist_split_prediction_bands(cde_fit,
xTrain=data_I2$x,
yTrain = data_I2$y,
xTest=data_test_aux$x,
alpha=alpha,median=TRUE)
data_test <- generate_data(n=n_test,x=data_test_aux$x)
# Dist-split
bands_dist[[rep]] <- dist_split_prediction_bands_evalY(fit_dist_split,
yTest=data_test$y)
rep <- rep+1
gc()
}
}
dist_split[[n_each_index]] <- eval_prediction_bands(xTest=data_test$x,
bands_dist,
alpha=alpha)
dist_split[[n_each_index]]$n <- n_each_set_grid[n_each_index]
saveRDS(dist_split,file = paste0(folder,"dist_median_split.RDS"))
rm(bands_dist)
}
source("../requirements.R")
source("../base_functions.R")
folder <- "../rds/gaussian_het/"
dir.create(folder, showWarnings = FALSE)
# if x is given, only generate response again
generate_het_gaussian <- function(n,d,x=NULL)
{
if(is.null(x))
{
x=matrix(runif(n*d,-5,5),n,d)
}
# response
y=rnorm(nrow(x),x[,1],abs(x[,1]))
return(list(x=x,y=y))
}
n_fits <- 25 # total numer of I1 datasets
n_repetitions <- 250 # total numer of I2 datasets
n_each_set_grid <- c(200,500,1000,2500,5000) # size of I1 and I2
n_test <- 500 # to check coverage
d <- 20
k <- 100
percent_train <- 0.7
alpha <- 0.1
generate_data <- function(n,x=NULL) {generate_het_gaussian(n=n,d=d,x=x)}
data_test_aux <- generate_data(n=n_test) # used to fix x test
cd_split_global <- list()
cd_split_local <- list()
dist_split <- list()
quantile_split <- list()
reg_split <- list()
reg_split_w <- list()
for(n_each_index in 1:length(n_each_set_grid))
{
print(n_each_index/length(n_each_set_grid))
rep <- 1
bands_global <- list()
bands_local <- list()
bands_dist <- list()
bands_quantile <- list()
bands_reg <- list()
bands_reg_w <- list()
for(n_fits_index in 1:n_fits)
{
cat(".")
data_I1 <- generate_data(n=n_each_set_grid[n_each_index])
which_train <- sample(1:length(data_I1$y),length(data_I1$y)*percent_train)
cde_fit <- fit_density_forest(xTrain=data_I1$x[which_train,,drop=FALSE],
yTrain = data_I1$y[which_train,drop=FALSE],
xValidation=data_I1$x[-which_train,,drop=FALSE],
yValidation = data_I1$y[-which_train,drop=FALSE])
for(ll in 1:n_repetitions)
{
data_I2 <- generate_data(n=n_each_set_grid[n_each_index])
pred_I2 <- predict(cde_fit,data_I2$x)
t_grid <- seq(0,max(pred_I2$CDE),length.out = 500)
# Dist-split
fit_dist_split <- dist_split_prediction_bands(cde_fit,
xTrain=data_I2$x,
yTrain = data_I2$y,
xTest=data_test_aux$x,
alpha=alpha,median=TRUE)
data_test <- generate_data(n=n_test,x=data_test_aux$x)
# Dist-split
bands_dist[[rep]] <- dist_split_prediction_bands_evalY(fit_dist_split,
yTest=data_test$y)
rep <- rep+1
gc()
}
}
dist_split[[n_each_index]] <- eval_prediction_bands(xTest=data_test$x,
bands_dist,
alpha=alpha)
dist_split[[n_each_index]]$n <- n_each_set_grid[n_each_index]
saveRDS(dist_split,file = paste0(folder,"dist_median_split.RDS"))
rm(bands_dist)
}
